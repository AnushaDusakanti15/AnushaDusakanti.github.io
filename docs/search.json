[
  {
    "objectID": "quali.html",
    "href": "quali.html",
    "title": "EDUCATION",
    "section": "",
    "text": "I have completed my education from the below institutions:\n\nB.Tech in Computer Science and Engineering from MLR Institute of Technology, with 7.4 CGPA, in 2022\n12th grade from Narayana Junior College, with 94.2%, in 2018\n10th grade from Silver Oaks International School, with 9.8 CGPA, in 2016.\n\nGo back to home."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "WORK EXPERIENCE",
    "section": "",
    "text": "I completed an internship with Virtusa Consulting Services Pvt Ltd for 4 months.\nI worked as an Associate Engineer at Virtusa Consulting Services Pvt Ltd for 1.6 years.\n\nDuring this whole tenure at Virtusa, I was able to learn about AWS services and worked on a project and on a POC.\n\nPOC\nPeriod: October 2023 – December 2023\nRole: Developer\nI worked on a POC, which requires extraction of data from the source database using SQL and converting existing Java code to python code and migrating that data into AWS cloud.\nBMO – Digital Core B2B – TIBCO BW to AWS\nPeriod: January 2023 – September 2023\nRole: Developer\nMigrating TIBCO on-premises API from on-premises servers to AWS Cloud Environment. This migration requires understanding the business logic and creating the request and response structure of the API from the TIBCO Designer and understanding the design given by the AWS architect.\n\nGo back to home."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anusha Dusakanti",
    "section": "",
    "text": "Hi everyone, I am Anusha Dusakanti and welcome to my website.\nI am currently pursuing my Masters in Data Analytics Engineering at George Mason University.\nMy passion to learn about data and its applications, made me choose this course for my Masters. I am confident that my proficiency in Java, Python, AWS will aid me in completing this course and I am looking forward to learn about R and using it for data visualizations.\nI submitted a redesign project as part of my ‘Applied Statistics and Visualization for Analytics’ course. Please visit the Redesigning Project page to learn more about it.\nTo know about my work experience, please navigate to Work Experience."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Anusha Dusakanti",
    "section": "",
    "text": "Hi everyone, I am Anusha Dusakanti and welcome to my website.\nI am currently pursuing my Masters in Data Analytics Engineering at George Mason University.\nMy passion to learn about data and its applications, made me choose this course for my Masters. I am confident that my proficiency in Java, Python, AWS will aid me in completing this course and I am looking forward to learn about R and using it for data visualizations.\nI submitted a redesign project as part of my ‘Applied Statistics and Visualization for Analytics’ course. Please visit the Redesigning Project page to learn more about it.\nTo know about my work experience, please navigate to Work Experience."
  },
  {
    "objectID": "Redesign.html",
    "href": "Redesign.html",
    "title": "REDESIGNING PROJECT",
    "section": "",
    "text": "Group 19:\n\n\nAnusha Dusakanti\n\n\nSumrah Shakeel\nBelow is a redesigning project on which my classmate, Sumrah Shakeel and I worked on for the Applied Statistics & Visualization for Analytics (STAT515) class taken at George Mason University. This project involves us finding a bad graph and redesigning it to display meaningful visualizations.\nThe graph we selected to redesign is a line chart that shows the Unemployment Rate from 1968-2022 in Japan. The amount of information on this graph is quite a lot and the viewer can barely pinpoint the percentages mentioned here.\n\n\n\nStrengths: Easy to Identify Trends\n\n\nWeakness: Cluttered, Overwhelming to the User, Difficult to Understand\n\n\nVisualization 1: Grouped Bar Chart for Youth Unemployment Rate during Employment Ice Age (1993-2004) in Japan\nWe redesigned the above graph as a bar chart, by considering the unemployment rate of youth(ages 15-24) during the Employment Ice Age period.\nFrom the graph below it is evident that the unemployment rate has steadily increased during the Employment Ice Age period. For all the person, it peaked in 2003 at 10.16%, for men it peaked in 2003 at 11.56%, and for women it peaked in 2001 at 8.71%. In 1994, which was the beginning of the Ice Age, the unemployment rate was the lowest for all people, men and women.\n\nStrengths: Easy to Compare Different Variables; Easy to Interpret\n\n\nWeakness: Difficult to Display Multiple Variables; Cannot Display Large Data on the x-axis\n\n\n\n\n\n\n\n\n\nVisualization 2: Multi-line plot of different genders, during the Employment Ice Age (1993-2004) in Japan\nOur next redesign is a multi-line plot with facets, with each facet representing a different gender and each line representing a trend during Japan’s Employment Ice Age (1993-2004). It is easy to see that the men’s unemployment rate has steadily increased until 2003, whilst the women’s unemployement rate has had short periods of decline.\n\nStrengths: Clearly distinguish between different genders; It displays trends over the years; It’s interactive\n\n\nWeakness: Line overlap caused by too many data points per category may hide patterns, making it difficult to detect distinct trends.\n\n\n\n\n\n\n\n\n\nVisualization 3: Line Chart showing Relationship between GDP Growth Rate and Youth Unemployment Rate in Japan\nFrom the line chart below, it looks like the Gross Domestic Product (GDP) growth rate has slowed down over years and the negative percentage indicates the recession that Japan was hit with. Unemployment rate was initially under 5% until the early 1990s and the then it started increasing and eventually peaked in 2003 reaching almost 10.16%. After this the unemployment rate started decreasing at a steady rate.\n\nStrengths: Easy to understand trends; Easy to correlate\n\n\nWeakness: Too much information can be overwhelming; Outliers can skew the data\n\n\n\n\n\n\n\n\n\nVisualization 4: World map with Youth Unemployment Rate of different countries in 2022\nThis last visualization depicts a world map showing the unemployment rates of various countries in 2022. We ranked the countries according to their unemployment rates and plotted them on a world map. The color legend ranges from red (Japan) representing the highest rated country to yellow (Greece), representing the lowest rated country.\n\nStrengths: Easy to identify the countries; Easy to compare rankings\n\n\nWeakness: Trends for specific countries cannot be shown.\n\n\n\n\n\n\nThe video below includes Sumrah and I walking you through the designing process of selecting a bad graph and then redesigning it into 4 different meaningful and interactive visualizations.\n\n\n\n\n\n\n\n\nReferences:\nOecd. (n.d.). LFS by sex and age - indicators. Organisation for Economic Co-operation and Development. https://stats.oecd.org/Index.aspx?DataSetCode=LFS_SEXAGE_I_R#\nOshio, T. (2020, September 5). Lingering Impact of Starting Working Life During a Recession: Health Outcomes of Survivors of the “Employment Ice Age” (1993–2004) in Japan. Journal of Epidemiology. https://doi.org/10.2188/jea.JE20190121\nUnemployment rate in Japan. Wikimedia Commons. (n.d.-a). https://commons.wikimedia.org/wiki/File:Unemployment_rate_in_Japan.svg\nWorld Development Indicators. The World Bank. (n.d.). https://databank.worldbank.org/reports.aspx?source=2&series=NY.GDP.MKTP.KD.ZG&country=JPN#advancedDownloadOptions"
  },
  {
    "objectID": "Midproject.html",
    "href": "Midproject.html",
    "title": "MIDPROJECT CODE",
    "section": "",
    "text": "Importing the CSV File containing Raw Data of Unemployment Rate alone\n\nlfs_sexage_raw = read.csv(\"/Users/anushadusakanti/Documents/GitHub/AnushaDusakanti15.github.io/LFS_SEXAGE_I_R_05032024021852784.csv\")\n\nUnderstanding the structure of the data frame\n\nstr(lfs_sexage_raw)\n\n'data.frame':   312384 obs. of  15 variables:\n $ COUNTRY   : chr  \"AUS\" \"AUS\" \"AUS\" \"AUS\" ...\n $ Country   : chr  \"Australia\" \"Australia\" \"Australia\" \"Australia\" ...\n $ SEX       : chr  \"MW\" \"MW\" \"MW\" \"MW\" ...\n $ Sex       : chr  \"All persons\" \"All persons\" \"All persons\" \"All persons\" ...\n $ AGE       : int  1519 1519 1519 1519 1519 1519 1519 1519 1519 1519 ...\n $ Age       : chr  \"15 to 19\" \"15 to 19\" \"15 to 19\" \"15 to 19\" ...\n $ SERIES    : chr  \"EPR\" \"EPR\" \"EPR\" \"EPR\" ...\n $ Series    : chr  \"Employment/population ratio\" \"Employment/population ratio\" \"Employment/population ratio\" \"Employment/population ratio\" ...\n $ FREQUENCY : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ Frequency : chr  \"Annual\" \"Annual\" \"Annual\" \"Annual\" ...\n $ TIME      : int  1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 ...\n $ Time      : int  1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 ...\n $ Value     : num  58.5 56.7 57.2 55 54.1 ...\n $ Flag.Codes: logi  NA NA NA NA NA NA ...\n $ Flags     : logi  NA NA NA NA NA NA ...\n\n\nSub-setting the raw data to select Unemployment Rate data of Japanese Youth\n\nunemployment_rate_japan = subset(lfs_sexage_raw, Country == 'Japan' & Age == '15 to 24' & Series == 'Unemployment rate')\n\nunemployment_rate_japan\n\n      COUNTRY Country   SEX         Sex  AGE      Age SERIES            Series\n19496     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19497     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19498     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19499     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19500     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19501     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19502     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19503     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19504     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19505     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19506     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19507     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19508     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19509     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19510     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19511     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19512     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19513     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19514     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19515     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19516     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19517     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19518     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19519     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19520     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19521     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19522     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19523     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19524     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19525     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19526     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19527     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19528     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19529     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19530     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19531     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19532     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19533     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19534     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19535     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19536     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19537     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19538     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19539     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19540     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19541     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19542     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19543     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19544     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19545     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19546     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19547     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19548     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19549     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19550     JPN   Japan    MW All persons 1524 15 to 24     UR Unemployment rate\n19551     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19552     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19553     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19554     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19555     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19556     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19557     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19558     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19559     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19560     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19561     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19562     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19563     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19564     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19565     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19566     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19567     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19568     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19569     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19570     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19571     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19572     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19573     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19574     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19575     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19576     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19577     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19578     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19579     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19580     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19581     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19582     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19583     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19584     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19585     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19586     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19587     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19588     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19589     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19590     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19591     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19592     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19593     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19594     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19595     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19596     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19597     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19598     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19599     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19600     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19601     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19602     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19603     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19604     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19605     JPN   Japan   MEN         Men 1524 15 to 24     UR Unemployment rate\n19606     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19607     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19608     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19609     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19610     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19611     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19612     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19613     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19614     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19615     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19616     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19617     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19618     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19619     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19620     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19621     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19622     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19623     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19624     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19625     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19626     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19627     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19628     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19629     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19630     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19631     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19632     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19633     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19634     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19635     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19636     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19637     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19638     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19639     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19640     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19641     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19642     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19643     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19644     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19645     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19646     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19647     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19648     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19649     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19650     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19651     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19652     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19653     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19654     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19655     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19656     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19657     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19658     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19659     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n19660     JPN   Japan WOMEN       Women 1524 15 to 24     UR Unemployment rate\n      FREQUENCY Frequency TIME Time     Value Flag.Codes Flags\n19496         A    Annual 1968 1968  1.875000         NA    NA\n19497         A    Annual 1969 1969  1.828154         NA    NA\n19498         A    Annual 1970 1970  1.983769         NA    NA\n19499         A    Annual 1971 1971  2.129547         NA    NA\n19500         A    Annual 1972 1972  2.390057         NA    NA\n19501         A    Annual 1973 1973  2.344546         NA    NA\n19502         A    Annual 1974 1974  2.474691         NA    NA\n19503         A    Annual 1975 1975  3.048780         NA    NA\n19504         A    Annual 1976 1976  3.129074         NA    NA\n19505         A    Annual 1977 1977  3.537415         NA    NA\n19506         A    Annual 1978 1978  3.755216         NA    NA\n19507         A    Annual 1979 1979  3.394625         NA    NA\n19508         A    Annual 1980 1980  3.576538         NA    NA\n19509         A    Annual 1981 1981  4.011461         NA    NA\n19510         A    Annual 1982 1982  3.917379         NA    NA\n19511         A    Annual 1983 1983  4.526749         NA    NA\n19512         A    Annual 1984 1984  4.904632         NA    NA\n19513         A    Annual 1985 1985  4.774898         NA    NA\n19514         A    Annual 1986 1986  5.165563         NA    NA\n19515         A    Annual 1987 1987  5.235602         NA    NA\n19516         A    Annual 1988 1988  4.853129         NA    NA\n19517         A    Annual 1989 1989  4.455446         NA    NA\n19518         A    Annual 1990 1990  4.316547         NA    NA\n19519         A    Annual 1991 1991  4.462243         NA    NA\n19520         A    Annual 1992 1992  4.357542         NA    NA\n19521         A    Annual 1993 1993  5.105438         NA    NA\n19522         A    Annual 1994 1994  5.450501         NA    NA\n19523         A    Annual 1995 1995  6.094808         NA    NA\n19524         A    Annual 1996 1996  6.727480         NA    NA\n19525         A    Annual 1997 1997  6.643357         NA    NA\n19526         A    Annual 1998 1998  7.720145         NA    NA\n19527         A    Annual 1999 1999  9.275731         NA    NA\n19528         A    Annual 2000 2000  9.198423         NA    NA\n19529         A    Annual 2001 2001  9.712722         NA    NA\n19530         A    Annual 2002 2002 10.043042         NA    NA\n19531         A    Annual 2003 2003 10.164425         NA    NA\n19532         A    Annual 2004 2004  9.472050         NA    NA\n19533         A    Annual 2005 2005  8.647799         NA    NA\n19534         A    Annual 2006 2006  8.000000         NA    NA\n19535         A    Annual 2007 2007  7.704918         NA    NA\n19536         A    Annual 2008 2008  7.239057         NA    NA\n19537         A    Annual 2009 2009  9.075044         NA    NA\n19538         A    Annual 2010 2010  9.222423         NA    NA\n19539         A    Annual 2011 2011  8.023483         NA    NA\n19540         A    Annual 2012 2012  7.945736         NA    NA\n19541         A    Annual 2013 2013  6.883365         NA    NA\n19542         A    Annual 2014 2014  6.285714         NA    NA\n19543         A    Annual 2015 2015  5.523810         NA    NA\n19544         A    Annual 2016 2016  5.118830         NA    NA\n19545         A    Annual 2017 2017  4.595588         NA    NA\n19546         A    Annual 2018 2018  3.760684         NA    NA\n19547         A    Annual 2019 2019  3.660566         NA    NA\n19548         A    Annual 2020 2020  4.576271         NA    NA\n19549         A    Annual 2021 2021  4.623288         NA    NA\n19550         A    Annual 2022 2022  4.203152         NA    NA\n19551         A    Annual 1968 1968  1.867572         NA    NA\n19552         A    Annual 1969 1969  1.916376         NA    NA\n19553         A    Annual 1970 1970  2.061856         NA    NA\n19554         A    Annual 1971 1971  2.325581         NA    NA\n19555         A    Annual 1972 1972  2.669039         NA    NA\n19556         A    Annual 1973 1973  2.509653         NA    NA\n19557         A    Annual 1974 1974  2.736842         NA    NA\n19558         A    Annual 1975 1975  3.456221         NA    NA\n19559         A    Annual 1976 1976  3.448276         NA    NA\n19560         A    Annual 1977 1977  3.957784         NA    NA\n19561         A    Annual 1978 1978  4.359673         NA    NA\n19562         A    Annual 1979 1979  3.631285         NA    NA\n19563         A    Annual 1980 1980  3.977273         NA    NA\n19564         A    Annual 1981 1981  4.237288         NA    NA\n19565         A    Annual 1982 1982  4.201681         NA    NA\n19566         A    Annual 1983 1983  4.594595         NA    NA\n19567         A    Annual 1984 1984  4.851752         NA    NA\n19568         A    Annual 1985 1985  4.838710         NA    NA\n19569         A    Annual 1986 1986  5.235602         NA    NA\n19570         A    Annual 1987 1987  5.426357         NA    NA\n19571         A    Annual 1988 1988  5.050505         NA    NA\n19572         A    Annual 1989 1989  4.679803         NA    NA\n19573         A    Annual 1990 1990  4.513064         NA    NA\n19574         A    Annual 1991 1991  4.719101         NA    NA\n19575         A    Annual 1992 1992  4.575163         NA    NA\n19576         A    Annual 1993 1993  4.935622         NA    NA\n19577         A    Annual 1994 1994  5.591398         NA    NA\n19578         A    Annual 1995 1995  6.113537         NA    NA\n19579         A    Annual 1996 1996  6.798246         NA    NA\n19580         A    Annual 1997 1997  6.935123         NA    NA\n19581         A    Annual 1998 1998  8.158508         NA    NA\n19582         A    Annual 1999 1999 10.319410         NA    NA\n19583         A    Annual 2000 2000 10.432570         NA    NA\n19584         A    Annual 2001 2001 10.666667         NA    NA\n19585         A    Annual 2002 2002 11.325967         NA    NA\n19586         A    Annual 2003 2003 11.560694         NA    NA\n19587         A    Annual 2004 2004 10.638298         NA    NA\n19588         A    Annual 2005 2005  9.876543         NA    NA\n19589         A    Annual 2006 2006  8.805031         NA    NA\n19590         A    Annual 2007 2007  8.280255         NA    NA\n19591         A    Annual 2008 2008  7.894737         NA    NA\n19592         A    Annual 2009 2009 10.069444         NA    NA\n19593         A    Annual 2010 2010 10.431655         NA    NA\n19594         A    Annual 2011 2011  8.949416         NA    NA\n19595         A    Annual 2012 2012  8.745247         NA    NA\n19596         A    Annual 2013 2013  7.604563         NA    NA\n19597         A    Annual 2014 2014  7.116105         NA    NA\n19598         A    Annual 2015 2015  5.947955         NA    NA\n19599         A    Annual 2016 2016  5.714286         NA    NA\n19600         A    Annual 2017 2017  4.693141         NA    NA\n19601         A    Annual 2018 2018  4.054054         NA    NA\n19602         A    Annual 2019 2019  3.947368         NA    NA\n19603         A    Annual 2020 2020  5.000000         NA    NA\n19604         A    Annual 2021 2021  5.084746         NA    NA\n19605         A    Annual 2022 2022  4.878049         NA    NA\n19606         A    Annual 1968 1968  1.883239         NA    NA\n19607         A    Annual 1969 1969  1.730769         NA    NA\n19608         A    Annual 1970 1970  1.897533         NA    NA\n19609         A    Annual 1971 1971  1.904762         NA    NA\n19610         A    Annual 1972 1972  2.066116         NA    NA\n19611         A    Annual 1973 1973  2.159827         NA    NA\n19612         A    Annual 1974 1974  2.173913         NA    NA\n19613         A    Annual 1975 1975  2.590674         NA    NA\n19614         A    Annual 1976 1976  2.770083         NA    NA\n19615         A    Annual 1977 1977  3.089888         NA    NA\n19616         A    Annual 1978 1978  3.125000         NA    NA\n19617         A    Annual 1979 1979  3.151862         NA    NA\n19618         A    Annual 1980 1980  3.170029         NA    NA\n19619         A    Annual 1981 1981  3.779070         NA    NA\n19620         A    Annual 1982 1982  3.623188         NA    NA\n19621         A    Annual 1983 1983  4.456825         NA    NA\n19622         A    Annual 1984 1984  4.958678         NA    NA\n19623         A    Annual 1985 1985  4.709141         NA    NA\n19624         A    Annual 1986 1986  5.093834         NA    NA\n19625         A    Annual 1987 1987  5.039788         NA    NA\n19626         A    Annual 1988 1988  4.651163         NA    NA\n19627         A    Annual 1989 1989  4.228856         NA    NA\n19628         A    Annual 1990 1990  4.116223         NA    NA\n19629         A    Annual 1991 1991  4.195804         NA    NA\n19630         A    Annual 1992 1992  4.128440         NA    NA\n19631         A    Annual 1993 1993  5.287356         NA    NA\n19632         A    Annual 1994 1994  5.299539         NA    NA\n19633         A    Annual 1995 1995  6.074766         NA    NA\n19634         A    Annual 1996 1996  6.650831         NA    NA\n19635         A    Annual 1997 1997  6.326034         NA    NA\n19636         A    Annual 1998 1998  7.250000         NA    NA\n19637         A    Annual 1999 1999  8.157895         NA    NA\n19638         A    Annual 2000 2000  7.880435         NA    NA\n19639         A    Annual 2001 2001  8.707865         NA    NA\n19640         A    Annual 2002 2002  8.656716         NA    NA\n19641         A    Annual 2003 2003  8.668731         NA    NA\n19642         A    Annual 2004 2004  8.253968         NA    NA\n19643         A    Annual 2005 2005  7.371795         NA    NA\n19644         A    Annual 2006 2006  7.166124         NA    NA\n19645         A    Annual 2007 2007  7.094595         NA    NA\n19646         A    Annual 2008 2008  6.551724         NA    NA\n19647         A    Annual 2009 2009  8.070175         NA    NA\n19648         A    Annual 2010 2010  8.000000         NA    NA\n19649         A    Annual 2011 2011  7.086614         NA    NA\n19650         A    Annual 2012 2012  7.114625         NA    NA\n19651         A    Annual 2013 2013  6.153846         NA    NA\n19652         A    Annual 2014 2014  5.426357         NA    NA\n19653         A    Annual 2015 2015  5.078125         NA    NA\n19654         A    Annual 2016 2016  4.494382         NA    NA\n19655         A    Annual 2017 2017  4.494382         NA    NA\n19656         A    Annual 2018 2018  3.460208         NA    NA\n19657         A    Annual 2019 2019  3.367003         NA    NA\n19658         A    Annual 2020 2020  4.137931         NA    NA\n19659         A    Annual 2021 2021  4.152249         NA    NA\n19660         A    Annual 2022 2022  3.521127         NA    NA\n\n\nKeeping only the required columns, rounding the Unemployment Rate to 2 digits, renaming columns to appropriate names and resetting the row numbers\n\nsuppressMessages(library(dplyr))\nunemployment_rate_japan = select(unemployment_rate_japan, Sex, Time, Value)\nunemployment_rate_japan$Value = round(unemployment_rate_japan$Value, digits = 2)\nunemployment_rate_japan &lt;- unemployment_rate_japan %&gt;% \n  rename(Year = Time)\n\nrow.names(unemployment_rate_japan) = NULL\n\nSelecting only the Unemployment Ice Age Years and resetting the row numbers\n\ndata1993_2004 = subset(unemployment_rate_japan, Year &gt;= 1993 & Year  &lt;= 2004)\nrow.names(data1993_2004) = NULL\n\nTransposing the Unemployment Ice Age data by pivoting the table from long to wide.\n\nsuppressMessages(library(tidyr))\ntransposed_1993_2004 = pivot_wider(data1993_2004, names_from = \"Sex\", values_from = \"Value\")\n\nGrouping the transposed data by Year and renaming the column containing spaces in between\n\ngrouped_1993_2004 = transposed_1993_2004 %&gt;%\n  group_by(Year)\ngrouped_1993_2004 = grouped_1993_2004 %&gt;% \n  rename(All.persons = 'All persons')\n\nCreating an interactive bar plot with the manipulated data frame to portray the population during Unemployment Ice Age\n\nsuppressMessages(library(plotly))\n\nbar_plt_1993_2004 = plot_ly(grouped_1993_2004, x = ~Year, y = ~All.persons, type = 'bar', name = 'All persons',\n             marker = list(color = 'green')) %&gt;% \n    add_trace(y = ~Men, name = 'Men', \n              marker = list(color = 'blue')) %&gt;%\n    add_trace(y = ~Women, name = 'Women', \n              marker = list(color = 'red')) %&gt;%\n    layout(xaxis = list(title = \"Years\", tickmode='linear', tickangle = -90),\n           yaxis = list(title = \"Youth Unemployment Rate (%)\"),\n           title = 'Youth Unemployment Rate during Ice Age \\n Japan (1993-2004)',\n            barmode = 'group')\nbar_plt_1993_2004\n\n\n\n\n\nRedesigning the bar plot and creating a interactive line plot to portray the population with separate graphs during Unemployment Ice Age\n\nsuppressMessages(library(ggplot2))\nsuppressMessages(library(plotly))\ncc &lt;- c(\"green\",\"blue\", \"red\")\n\ngraph &lt;- ggplot(data1993_2004, aes(x = Year, y = Value, fill = Sex, group=1,\n                                  text = paste(\"Sex:\", Sex, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;UR:\", Value))) +\n  geom_line(color=\"black\") +\n  geom_point(shape = 21, size = 2.8) + \n  labs(x = \"Year\", y = \"Youth Unemployment Rate(%)\", title = \"Youth Unemployment Rate in Japan(1993-2004)\") +\n  facet_wrap(~ Sex) +\n  scale_fill_manual(values = cc) +\n  labs(fill = \"Population\") +\n  theme_grey() +\n  scale_x_continuous(breaks = seq(1993, 2004, by = 3))\n\nggplotly(graph, tooltip = \"text\")\n\n\n\n\n\nImporting the CSV file containing Japan’s Anual GDP Growth Rate obtained from https://databank.worldbank.org/reports.aspx?source=2&series=NY.GDP.MKTP.KD.ZG&country=JPN#advancedDownloadOptions\n\ngdp_annual_1960 = read.csv(\"/Users/anushadusakanti/Documents/GitHub/AnushaDusakanti15.github.io/Japan_GDP_Annual_Growth.csv\")\n\nUnderstanding the structure of the dataset\n\nstr(gdp_annual_1960)\n\n'data.frame':   72 obs. of  7 variables:\n $ Series.Name : chr  \"GDP growth (annual %)\" \"GDP growth (annual %)\" \"GDP growth (annual %)\" \"GDP growth (annual %)\" ...\n $ Series.Code : chr  \"NY.GDP.MKTP.KD.ZG\" \"NY.GDP.MKTP.KD.ZG\" \"NY.GDP.MKTP.KD.ZG\" \"NY.GDP.MKTP.KD.ZG\" ...\n $ Country.Name: chr  \"Japan\" \"Japan\" \"Japan\" \"Japan\" ...\n $ Country.Code: chr  \"JPN\" \"JPN\" \"JPN\" \"JPN\" ...\n $ Time        : chr  \"1960\" \"1961\" \"1962\" \"1963\" ...\n $ Time.Code   : chr  \"YR1960\" \"YR1961\" \"YR1962\" \"YR1963\" ...\n $ Value       : chr  \"..\" \"12.0435364080116\" \"8.90897299558067\" \"8.47364238279391\" ...\n\n\nSelecting only the columns required\n\ngdp_annual_1960 = select(gdp_annual_1960, Time, Value)\n\nKeeping only the meaningful rows\n\ngdp_annual = slice(gdp_annual_1960, 9:63)\n\nData Manipulation - converting character columns to numeric, rounding off percentage to 2 digits, and renaming columns appropriately\n\ngdp_annual$Time = as.numeric(gdp_annual$Time)\ngdp_annual$Value = as.numeric(gdp_annual$Value)\ngdp_annual$Value = round(gdp_annual$Value, digits = 2)\ngdp_annual = gdp_annual %&gt;% \n  rename(GDP_Annual_Growth = 'Value')\ngdp_annual = gdp_annual %&gt;% \n  rename(Year = 'Time')\n\nSubsetting the population to ‘All persons’ to get an overall unemployment outlook from the previously created Unemployment Rate data of Japanese Youth data frame\n\nunemployment_all_persons = subset(unemployment_rate_japan, Sex == 'All persons')\n\nData Manipulation - converting character columns to numeric, rounding off percetnage to 2 digits, and renaming columns appropriately\n\nunemployment_time_value = select(unemployment_all_persons, Year, Value)\nunemployment_time_value = unemployment_time_value %&gt;% \n  rename(Unemployment_Rate = 'Value')\n\nDoing an inner join on the GDP data frame and Unemployment Years data frame with the common column ‘Year’\n\ngdp_unemployment = merge(x = gdp_annual, y = unemployment_time_value, by = 'Year')\ngdp_unemployment\n\n   Year GDP_Annual_Growth Unemployment_Rate\n1  1968             12.88              1.88\n2  1969             12.48              1.83\n3  1970              2.45              1.98\n4  1971              4.70              2.13\n5  1972              8.41              2.39\n6  1973              8.03              2.34\n7  1974             -1.23              2.47\n8  1975              3.09              3.05\n9  1976              3.97              3.13\n10 1977              4.39              3.54\n11 1978              5.27              3.76\n12 1979              5.48              3.39\n13 1980              2.82              3.58\n14 1981              4.26              4.01\n15 1982              3.28              3.92\n16 1983              3.63              4.53\n17 1984              4.41              4.90\n18 1985              5.16              4.77\n19 1986              3.29              5.17\n20 1987              4.65              5.24\n21 1988              6.66              4.85\n22 1989              4.93              4.46\n23 1990              4.84              4.32\n24 1991              3.52              4.46\n25 1992              0.90              4.36\n26 1993             -0.46              5.11\n27 1994              1.08              5.45\n28 1995              2.63              6.09\n29 1996              3.13              6.73\n30 1997              0.98              6.64\n31 1998             -1.27              7.72\n32 1999             -0.33              9.28\n33 2000              2.76              9.20\n34 2001              0.39              9.71\n35 2002              0.04             10.04\n36 2003              1.54             10.16\n37 2004              2.19              9.47\n38 2005              1.80              8.65\n39 2006              1.37              8.00\n40 2007              1.48              7.70\n41 2008             -1.22              7.24\n42 2009             -5.69              9.08\n43 2010              4.10              9.22\n44 2011              0.02              8.02\n45 2012              1.37              7.95\n46 2013              2.01              6.88\n47 2014              0.30              6.29\n48 2015              1.56              5.52\n49 2016              0.75              5.12\n50 2017              1.68              4.60\n51 2018              0.64              3.76\n52 2019             -0.40              3.66\n53 2020             -4.15              4.58\n54 2021              2.56              4.62\n55 2022              0.95              4.20\n\n\nCreating an interactive line chart to depict the relationship between GDP Growth Rate and Unemployment Rate\n\nline_gdp_unemp = plot_ly(gdp_unemployment, x = ~Year) %&gt;% \n    add_trace(y = ~GDP_Annual_Growth, name = 'GDP Annual Growth (in Percentage)', type = 'scatter', mode = 'lines') %&gt;% \n    add_trace(y = ~Unemployment_Rate, name = 'Unemployment Rate (in Percentage)', type = 'scatter', mode = 'lines') %&gt;%\n    layout(xaxis = list(title = \"Years\", tickmode='linear', tickangle = -90),\n           yaxis = list(title = \"Percentage\"),\n           title = 'GDP Growth Rate vs Unemployment Rate \\n Japan (1968-2022)')\nline_gdp_unemp\n\n\n\n\n\nCreating a dataset with unemployment rate of all countries in the year 2022\n\ncountries_info = subset(lfs_sexage_raw, Age == \"15 to 24\" & Series == \"Unemployment rate\" & Time == \"2022\" & Sex == \"All persons\")\n\ncountries_info &lt;- countries_info[order(countries_info$Value), ]\n\nrow.names(countries_info) = NULL\n\nPlotting an interactive map with unemployment rate of all countries in the year 2022\n\nsuppressWarnings({\n  color_scale &lt;- colorRamp(c(\"red\", \"orange\", \"yellow\"))\n  \n  min_value &lt;- ifelse(length(countries_info$Value) &gt; 0, min(countries_info$Value, na.rm = TRUE), NA)\n  max_value &lt;- ifelse(length(countries_info$Value) &gt; 0, max(countries_info$Value, na.rm = TRUE), NA)\n\n  rounded_ticks &lt;- round(seq(min_value, max_value, length.out = 5), 2)\n  \n  fig &lt;- plot_ly(countries_info,\n                 type = \"scattergeo\",\n                 mode = \"markers\",\n                 locationmode = \"country names\",\n                 locations = ~Country,\n                 color = ~Value,\n                 text = ~paste(\"Country: \", Country, \"&lt;br&gt;UR: \", round(Value, 2)),\n                 colors = color_scale,\n                 marker = list(size = 10)) %&gt;%\n    layout(title = \"Youth Unemployment Rate in 2022\",\n           geo = list(\n             showframe = FALSE,\n             projection = list(type = 'mercator'),\n             bgcolor = \"lightblue\",\n             showcoastlines = TRUE,\n             showland = TRUE,\n             landcolor = \"lightgreen\",\n             showocean = TRUE,\n             oceancolor = \"lightblue\"\n           )) %&gt;%\n    colorbar(title = \"Youth Unemployment Rate(%)\",\n             tickvals = seq(min_value, max_value, length.out = 5),\n             ticktext = rounded_ticks)\n\n  fig\n})"
  },
  {
    "objectID": "Finalprocodes.html",
    "href": "Finalprocodes.html",
    "title": "FINAL PROJECT CODE",
    "section": "",
    "text": "packages &lt;- c( \"randomForest\",\"tree\",\"dplyr\", \"tidyverse\", \"plotly\", \"psych\", \"ISLR\", \"leaps\", \"reshape2\",\"readr\")\n\nfor (package in packages) {\n  if (!(package %in% installed.packages())) {\n    install.packages(package, dependencies = TRUE)\n  }\n}\n# Load libraries\nsuppressMessages({library(dplyr)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(psych)\nlibrary(ISLR)\nlibrary(leaps)\nlibrary(reshape2)\nlibrary(readr)\nlibrary(tree)\nlibrary(randomForest)\nlibrary(caret)})\n\n\nurl &lt;- \"https://raw.githubusercontent.com/KyleWandel/STAT-515-Final-Project/main/breast-cancer-wisconsin.csv\"\ndf &lt;- read.table(url, header = TRUE, sep = \",\")\nstr(df)\n\n'data.frame':   699 obs. of  11 variables:\n $ id              : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...\n $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...\n $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...\n $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...\n $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...\n $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...\n $ barenuclei      : chr  \"1\" \"10\" \"2\" \"4\" ...\n $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...\n $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...\n $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...\n $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...\n\n\nTo make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed he response variable values to malignant (4) = 1 and benign (2) = 0.\nAfter cleaning the dataset, we looked at a summary statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).\n\n# remove ID column \ndf &lt;- subset(df, select = -id) \n# Change column type\ndf$barenuclei &lt;- as.integer(df$barenuclei) \n\nWarning: NAs introduced by coercion\n\n# Look for missing values\ncolSums(is.na(df)) \n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n               0                0                0                0 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n               0               16                0                0 \n         mitoses         benormal \n               0                0 \n\n# omit missing values\ndf_clean &lt;- na.omit(df) \nstr(df_clean) \n\n'data.frame':   683 obs. of  10 variables:\n $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...\n $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...\n $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...\n $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...\n $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...\n $ barenuclei      : int  1 10 2 4 1 10 10 1 1 1 ...\n $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...\n $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...\n $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...\n $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n# Change response variable to 1 and 0\ndf_clean$benormal &lt;- ifelse(df_clean$benormal == 4, 1, ifelse(df_clean$benormal == 2, 0, df_clean$benormal)) \nsummary(df_clean) \n\n clumpthickness   uniformcellsize  uniformcellshape  margadhesion  \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.: 1.00  \n Median : 4.000   Median : 1.000   Median : 1.000   Median : 1.00  \n Mean   : 4.442   Mean   : 3.151   Mean   : 3.215   Mean   : 2.83  \n 3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 5.000   3rd Qu.: 4.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n   epithelial       barenuclei     blandchromatin   normalnucleoli \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.00  \n Median : 2.000   Median : 1.000   Median : 3.000   Median : 1.00  \n Mean   : 3.234   Mean   : 3.545   Mean   : 3.445   Mean   : 2.87  \n 3rd Qu.: 4.000   3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n    mitoses          benormal     \n Min.   : 1.000   Min.   :0.0000  \n 1st Qu.: 1.000   1st Qu.:0.0000  \n Median : 1.000   Median :0.0000  \n Mean   : 1.603   Mean   :0.3499  \n 3rd Qu.: 1.000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :1.0000  \n\ntable(df_clean$benormal)\n\n\n  0   1 \n444 239 \n\n\nWe next wanted to identify if there were any patterns amongst the predictor variables in the dataset. First we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.\n\npairs.panels(df_clean)\n\n\n\n\nFor the first model we used the cleaned dataset and all of the variables.\n\nmodel_1 &lt;- glm(benormal ~ ., data = df_clean, family = binomial) \nsummary(model_1) \n\n\nCall:\nglm(formula = benormal ~ ., family = binomial, data = df_clean)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -10.10394    1.17488  -8.600  &lt; 2e-16 ***\nclumpthickness     0.53501    0.14202   3.767 0.000165 ***\nuniformcellsize   -0.00628    0.20908  -0.030 0.976039    \nuniformcellshape   0.32271    0.23060   1.399 0.161688    \nmargadhesion       0.33064    0.12345   2.678 0.007400 ** \nepithelial         0.09663    0.15659   0.617 0.537159    \nbarenuclei         0.38303    0.09384   4.082 4.47e-05 ***\nblandchromatin     0.44719    0.17138   2.609 0.009073 ** \nnormalnucleoli     0.21303    0.11287   1.887 0.059115 .  \nmitoses            0.53484    0.32877   1.627 0.103788    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 102.89  on 673  degrees of freedom\nAIC: 122.89\n\nNumber of Fisher Scoring iterations: 8\n\npar(mfrow = c(2, 2))\nplot(model_1)\n\n\n\n\nIn this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 122.89.\nNext, we wanted to create a new model after logging our variables.\n\nvariables_to_log &lt;- c(\"mitoses\", \"normalnucleoli\", \"blandchromatin\", \"epithelial\", \"margadhesion\", \"uniformcellshape\", \"uniformcellsize\",\"clumpthickness\") \ndf_log &lt;- df_clean \ndf_log[variables_to_log] &lt;- lapply(df_log[variables_to_log], log) \nmodel_2 &lt;- glm(benormal ~ ., data = df_log, family = binomial) \nsummary(model_2) \n\n\nCall:\nglm(formula = benormal ~ ., family = binomial, data = df_log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -9.31304    1.25953  -7.394 1.42e-13 ***\nclumpthickness    1.80864    0.59805   3.024  0.00249 ** \nuniformcellsize   0.50056    0.66380   0.754  0.45080    \nuniformcellshape  1.26038    0.74379   1.695  0.09016 .  \nmargadhesion      0.71750    0.39504   1.816  0.06933 .  \nepithelial       -0.04541    0.63630  -0.071  0.94310    \nbarenuclei        0.36117    0.09149   3.948 7.89e-05 ***\nblandchromatin    1.49565    0.61321   2.439  0.01473 *  \nnormalnucleoli    0.48867    0.35560   1.374  0.16938    \nmitoses           1.34541    0.74040   1.817  0.06920 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 107.02  on 673  degrees of freedom\nAIC: 127.02\n\nNumber of Fisher Scoring iterations: 8\n\npar(mfrow = c(2, 2))\nplot(model_2)\n\n\n\n\nFor this model the variables clumpthickness and barenuclei were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 127.02.\nBased on the AIC of these two models, the non-logged model performed better. Now lets try and simplify the model.\n\nmodel_3 &lt;- glm(benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin, data = df_clean, family = binomial) \nsummary(model_3) \n\n\nCall:\nglm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + \n    blandchromatin, family = binomial, data = df_clean)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -10.11370    1.03264  -9.794  &lt; 2e-16 ***\nclumpthickness   0.81166    0.12585   6.450 1.12e-10 ***\nmargadhesion     0.43412    0.11403   3.807 0.000141 ***\nbarenuclei       0.48136    0.08816   5.460 4.76e-08 ***\nblandchromatin   0.70154    0.15196   4.616 3.90e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 125.77  on 678  degrees of freedom\nAIC: 135.77\n\nNumber of Fisher Scoring iterations: 8\n\nmodel_3 \n\n\nCall:  glm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + \n    blandchromatin, family = binomial, data = df_clean)\n\nCoefficients:\n   (Intercept)  clumpthickness    margadhesion      barenuclei  blandchromatin  \n      -10.1137          0.8117          0.4341          0.4814          0.7015  \n\nDegrees of Freedom: 682 Total (i.e. Null);  678 Residual\nNull Deviance:      884.4 \nResidual Deviance: 125.8    AIC: 135.8\n\nanova(model_1, model_3, test = \"Chisq\") \n\nAnalysis of Deviance Table\n\nModel 1: benormal ~ clumpthickness + uniformcellsize + uniformcellshape + \n    margadhesion + epithelial + barenuclei + blandchromatin + \n    normalnucleoli + mitoses\nModel 2: benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       673     102.89                          \n2       678     125.78 -5  -22.886 0.0003549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs should be expected, creating a model using only the significant variables was worse at explaining the dataset. Using the Chisq test, we can also conclude that the more complex model is significantly better than the simpler model.\nUsing model_1 as the final model, lets see how accurate it is for prediction.\n\nglm.probs=predict(model_1,type=\"response\")\nglm.probs[1:10]\n\n          1           2           3           4           5           6 \n0.016046581 0.908808622 0.008137623 0.760934919 0.018166848 0.999973622 \n          7           8           9          10 \n0.056844170 0.004503358 0.011249056 0.006032371 \n\nglm.pred=rep(0,nrow(df_clean))\nglm.pred[glm.probs&gt;.5]=1\n\nglm.pred=as.factor(glm.pred)\ndf_clean$benormal=as.factor(df_clean$benormal)\ncm = table(glm.pred,df_clean$benormal)\nprint(cm)\n\n        \nglm.pred   0   1\n       0 434  11\n       1  10 228\n\n\nLooking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).\n\nDecision Tree and Random Forest Modeling\nAnother modeling type we used for trying to predict whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model.\nFirst, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.\n\ndf_clean$benormal=as.integer(df_clean$benormal)\nset.seed(2) \ntrain = sample(1:nrow(df_clean), nrow(df_clean)/2) \ntree.df_clean=tree(benormal~.,df_clean,subset=train) \ncv.df_clean=cv.tree(tree.df_clean) \nplot(cv.df_clean$size,cv.df_clean$dev,type='b')\n\n\n\n\nBased on these results, it is best to include 6 variables in each split. Knowing this, we created a random forest model at the desired variable split.\n\ndf_clean &lt;- na.omit(df) \ndf_clean$benormal &lt;- ifelse(df_clean$benormal == 4, 1, ifelse(df_clean$benormal == 2, 0, df_clean$benormal))\ncancer_rf=randomForest(benormal~.,data=df_clean,subset=train,mtry=6,importance=TRUE) \n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\nyhat.rf = predict(cancer_rf,newdata=df_clean[-train,]) \ndf_clean.test=df_clean[-train,\"benormal\"] \nmean((yhat.rf-df_clean.test)^2) #test set MSE \n\n[1] 0.03088145\n\ncancer_rf \n\n\nCall:\n randomForest(formula = benormal ~ ., data = df_clean, mtry = 6,      importance = TRUE, subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 0.0294727\n                    % Var explained: 86.76\n\nplot(cancer_rf) \n\n\n\nvarImpPlot(cancer_rf) \n\n\n\n\nThe variable that is most important to reduce the mean standard error (MSE) is barenuceli and the variables that were deemed the most important to include in the node splitting were uniformcellsize and uniformcellshape. Overall, the model is very good with 88.7% of the variables explained. Looking at the confusion matrix:\n\nglm.probs=predict(cancer_rf,df_clean,type=\"response\")\nglm.probs[1:10]\n\n            1             2             3             4             5 \n-6.311618e-16  7.780000e-01 -5.944134e-16  3.439000e-01  4.533333e-03 \n            6             7             8             9            10 \n 1.000000e+00  2.551000e-01 -6.267209e-16  2.292000e-01 -5.822010e-16 \n\nglm.pred=rep(0,nrow(df_clean))\nglm.pred[glm.probs&gt;.5]=1\n\nglm.pred=as.factor(glm.pred)\ndf_clean$benormal=as.factor(df_clean$benormal)\nconfusionMatrix(glm.pred,df_clean$benormal)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 438   9\n         1   6 230\n                                         \n               Accuracy : 0.978          \n                 95% CI : (0.964, 0.9877)\n    No Information Rate : 0.6501         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.9516         \n                                         \n Mcnemar's Test P-Value : 0.6056         \n                                         \n            Sensitivity : 0.9865         \n            Specificity : 0.9623         \n         Pos Pred Value : 0.9799         \n         Neg Pred Value : 0.9746         \n             Prevalence : 0.6501         \n         Detection Rate : 0.6413         \n   Detection Prevalence : 0.6545         \n      Balanced Accuracy : 0.9744         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nThe percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%).\n\n\nPrincipal Component Analysis\nWe mentioned earlier that some of our variables were correlated but none were significantly correlated to each other. Also, in some of our other models, some of the variables were not significantly correlated to predicting the response variable. To examine this further, we did a principal component analysis on the predictor variables too see if reducing the number of variables of a data set naturally comes at the expense of accuracy while not losing too much accuracy.\n\ndf_clean$benormal &lt;- as.integer(df_clean$benormal)\nstates=row.names(df_clean)\napply(df_clean, 2, mean)\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n        4.442167         3.150805         3.215227         2.830161 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n        3.234261         3.544656         3.445095         2.869693 \n         mitoses         benormal \n        1.603221         1.349927 \n\napply(df_clean, 2, var)\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n       7.9566944        9.3951130        8.9316153        8.2057165 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n       4.9421089       13.2776950        6.0010133        9.3187722 \n         mitoses         benormal \n       3.0021597        0.2278116 \n\n# There isn't much difference in the mean and variance, so we do not need to scale/standardize the variables.\n# Calculate the principal components\npr.out=prcomp(df_clean, scale=TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\npr.out$center\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n        4.442167         3.150805         3.215227         2.830161 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n        3.234261         3.544656         3.445095         2.869693 \n         mitoses         benormal \n        1.603221         1.349927 \n\nhead(pr.out$x,n = 10)\n\n         PC1         PC2         PC3         PC4         PC5         PC6\n1   1.632138 -0.10110899  0.53066017  0.04867467 -0.13967887  0.27266403\n2  -1.091952 -0.36764796 -0.37608283 -0.33727673  1.66159818 -0.49947533\n3   1.747215 -0.06835623 -0.05452903 -0.08442398 -0.05762203 -0.12479945\n4  -1.123580 -0.30561252  0.32020004  1.60389615 -0.54780477  0.16977170\n5   1.517047 -0.06200488 -0.04883144 -0.28957594 -0.09371811  0.57460157\n6  -5.173918 -1.36302344 -0.65367495  0.40382274  0.40838925  0.39318524\n7   1.250797 -0.53179083 -0.59909575 -1.16253763  0.26500705 -1.70934020\n8   1.817759  0.03688650 -0.35718994  0.11869407 -0.03111154 -0.03121061\n9   1.714243  2.28289475  0.16547803 -0.59033209 -0.12926420 -0.30699255\n10  1.749457  0.02462355  0.31567378  0.12332678  0.02349157  0.20775672\n           PC7         PC8         PC9        PC10\n1   0.32271338 -0.41613912  0.14987849 -0.01320316\n2  -0.76488363 -0.51190377  1.43717566 -0.18341430\n3   0.33212748 -0.22014329  0.14173141 -0.02990047\n4  -0.18141468  1.53841728  1.21084570 -0.32073360\n5   0.09877523 -0.42777805  0.09395639  0.02496084\n6   0.34574434  0.13590736  0.52396579 -0.01704468\n7   0.10416365  0.10310453  0.89835584 -0.27270693\n8   0.41671423  0.04563756  0.03920288  0.22753908\n9   0.16385020  0.27850361 -0.07680255 -0.01644927\n10  0.07179972 -0.02809058  0.05914645 -0.26772940\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\n(pr.var=pr.out$sdev^2) #variance of each PC\n\n [1] 6.73117831 0.79315381 0.54596815 0.46529869 0.38038147 0.31254327\n [7] 0.29602826 0.26121943 0.12634121 0.08788742\n\npve=pr.var/sum(pr.var)\npve\n\n [1] 0.673117831 0.079315381 0.054596815 0.046529869 0.038038147 0.031254327\n [7] 0.029602826 0.026121943 0.012634121 0.008788742\n\n# Scree plot\npar(mfrow =c(1,2))\nplot(pve, xlab=\"Principal Component\", \n     ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\nplot(cumsum(pve), xlab=\"Principal Component\",\n     ylab=\"Cumulative Proportion of Variance Explained\", \n     ylim=c(0,1),type='b')"
  },
  {
    "objectID": "Finalprocodes.html#iii.-limitations",
    "href": "Finalprocodes.html#iii.-limitations",
    "title": "FINAL PROJECT CODE",
    "section": "iii. Limitations",
    "text": "iii. Limitations\nThere were no major limitations for this analysis but there were a few of things that needed to be done to the dataset in order to clean and make the dataset usable for analysis. First, we had to identify and remove all missing rows from the dataset. Second, we had to transform the response variable to be \"0\" and \"1\". Finally, many of the variables seemed to show a left skew making us question if the variables should be transformed or not. \nThere were only 699 samples for the dataset and although a solid number of samples, more samples would lead to a more predictable conclusion."
  },
  {
    "objectID": "Finalprocodes.html#iv.-conclusion",
    "href": "Finalprocodes.html#iv.-conclusion",
    "title": "FINAL PROJECT CODE",
    "section": "iv. Conclusion",
    "text": "iv. Conclusion\nIt seems that a significantly accurate model to predict if a cancerous cell could be malignant using the measurements recorded in this dataset. To test this, we created multiple regression models, a random forest model, and a PCA analysis to understand the variables more thoroughly.\nIn the PCA test we determined that the dataset's variance could be explained by simplifying and using only 2 of 9 predictor variables. We then created multiple logistic regression models and compared them to each other to choose the best one. After variable transformation and selection, we determined the best model would be to use all the variables. The percent chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%). We then decided to try and create an optimal best random forest model, starting with a best fit decision tree model. The created model resulted in an overall accuracy rate of almost 98% with the percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%). Based on the two models we created, the best model to use would be the random forest model because of its high accuracy and predictability."
  },
  {
    "objectID": "Finalprocodes.html#references",
    "href": "Finalprocodes.html#references",
    "title": "FINAL PROJECT CODE",
    "section": "References",
    "text": "References\n[1] Wolberg,WIlliam. (1992). Breast Cancer Wisconsin (Original). UCI Machine Learning Repository. https://doi.org/10.24432/C5HP4Z."
  },
  {
    "objectID": "Final.html",
    "href": "Final.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "STAT 515 Final Project\nGroup Number: 2\nAnusha Dusakanti, Kyle Wandel, Sumrah Shakeel\n\n\nAbstract\nThe aim of this research is to identify patterns and indicators that could potentially predict the signs of malignant cancerous cells. Through meticulous analysis, the paper investigates various aspects, including a deep dive into the predictor variables and their relationship to each other and the response variable, the creation of various statistical models to predict cancerous cells and then finally a comparison of models to choose the best one. By scrutinizing these factors, this research hopes to make discovering breast cancer in patients easier and earlier.\n\n\nI. Introduction\nAmong all cancers, breast cancer is one of the most prevalent worldwide. Even if the pharmaceutical sector has made significant investments in the search for a permanent cure for this cancer, it still raises the need for more analysis to be done on breast cancer data so that a cancerous tumor can be caught in the early stages. We intended to figure out whether distinct characteristics of the tumor cells may be used to predict the prospective appearance of breast cancer in females. With nine predictor variables and an outcome variable that would indicate whether the tumor is categorized as malignant (cancerous) or benign (non-cancerous), this dataset seemed promising.\nThe dataset we choose to perform this research is from Dr. William Wolberg and his clinical studies from 1989 to 1991. This dataset is very well known and highly integrable due to the amount of research conducted using this data. Below is a list and description of the 9 predictor variables and the 1 response variable (benormal).\n\nclumpthickness: (1-10). Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers.\nuniformcellsize (1-10). Cancer cells tend to vary in size and shape.\nuniformcellshape (1-10). Cancer cells tend to vary in shape and size.\nmargadhesion: (1-10). Normal cells tend to stick together, while cancer cells tend to lose this ability, so the loss of adhesion is a sign of malignancy.\nepithelial: (1-10). It is related to the uniformity mentioned above. Epithelial cells that are significantly enlarged may be malignant.\nbarenuclei: (1-10). This term is used for nuclei not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumors.\nblandchromatin: (1-10). Describes a uniform “texture” of the nucleus seen in benign cells. In cancer cells, the chromatin tends to be more coarse and to form clumps.\nnormalnucleoli: (1-10). Nucleoli are small structures seen in the nucleus. In normal cells, the nucleolus is usually very small, if visible. The nucleoli become more prominent in cancer cells, and sometimes there are multiple.\nmitoses: (1-10). Cancer is essentially a disease of uncontrolled mitosis.\nbenormal: (2 or 4). Benign (non-cancerous) or malignant (cancerous) lump in a breast.\n\n\n\nII. Materials and Methods\nThe University of Wisconsin breast cancer data from William Wolberg has 699 observations and 10 variables, the first variable represents the ID of the sample and the last column “benormal” represents the classification/response variable (for benign, 4 for malignant).\nTo make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed the response variable values to malignant (4) = 1 and benign (2) = 0, then looked at summary statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).\nTo identify if there were any patterns among the predictor variables in the dataset, first we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.\n\n\n\n\n\nSome of the variables showed correlations to each other, but none were deemed significant by the corr.test() function. We noticed many of the variables exhibited a right skew with their means larger than their medians. We could potentially log() the variables to make them more uniform.\n\n\nLogistic Regression Model\nWe developed a logisitic regression model to predict if a cell was cancerous or not. As our outcome can only be one of two things (cell is malignant or benign) we should be using a classification model and logistic regression is a simple model which is much easier to set up and train initially than other machine learning models.\nFor the first model we used the cleaned dataset and all of the variables.\nModel 1-\n\n\n\n\n\nIn this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than 0.5. The overall model had an AIC of 122.89.\nNext, we wanted to create a new model after logging our variables.\nModel 2-\n\n\n\n\n\nFor this model the variables clumpthickness and barenuclei were considered the only variables that had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 127.02. \nBased on the AIC of these two models, the non-logged model performed better. Now lets try and simplify the model. \nModel 3-\n\n\n\n\n\nAs could be predicted, the model that included only the significant variables was worse at explaining the dataset. The complex model is definitely better than the simpler model, as demonstrated by the results of the Chisq test.\nFinally, evaluating how accurate model_1 is at prediction by using confusion matrix-\n\n\n\n\n\nLooking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).\n\n\nDecision Tree and Random Forest Modeling\nAnother modeling type we used for trying to predict whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model.First, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.\nFirst, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy."
  }
]