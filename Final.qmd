---
title: "FINAL PROJECT"
format: html
editor: visual
---

#### **STAT 515 Final Project**

Group Number: 2

Anusha Dusakanti, Kyle Wandel, Sumrah Shakeel

#### **Abstract**

The aim of this research is to identify patterns and indicators that could potentially predict the signs of malignant cancerous cells. Through meticulous analysis, the paper investigates various aspects, including a deep dive into the predictor variables and their relationship to each other and the response variable, the creation of various statistical models to predict cancerous cells and then finally a comparison of models to choose the best one. By scrutinizing these factors, this research hopes to make discovering breast cancer in patients easier and earlier.

#### **I. Introduction**

Among all cancers, breast cancer is one of the most prevalent worldwide. Even if the pharmaceutical sector has made significant investments in the search for a permanent cure for this cancer, it still raises the need for more analysis to be done on breast cancer data so that a cancerous tumor can be caught in the early stages. We intended to figure out whether distinct characteristics of the tumor cells may be used to predict the prospective appearance of breast cancer in females. With nine predictor variables and an outcome variable that would indicate whether the tumor is categorized as malignant (cancerous) or benign (non-cancerous), this dataset seemed promising.

The dataset we choose to perform this research is from Dr. William Wolberg and his clinical studies from 1989 to 1991. This dataset is very well known and highly integrable due to the amount of research conducted using this data. Below is a list and description of the 9 predictor variables and the 1 response variable (benormal).

-   **clumpthickness**: (1-10). Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers.

-   **uniformcellsize** (1-10). Cancer cells tend to vary in size and shape.

-   **uniformcellshape** (1-10). Cancer cells tend to vary in shape and size.

-   **margadhesion**: (1-10). Normal cells tend to stick together, while cancer cells tend to lose this ability, so the loss of adhesion is a sign of malignancy.

-   **epithelial**: (1-10). It is related to the uniformity mentioned above. Epithelial cells that are significantly enlarged may be malignant.

-   **barenuclei**: (1-10). This term is used for nuclei not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumors.

-   **blandchromatin**: (1-10). Describes a uniform “texture” of the nucleus seen in benign cells. In cancer cells, the chromatin tends to be more coarse and to form clumps.

-   **normalnucleoli**: (1-10). Nucleoli are small structures seen in the nucleus. In normal cells, the nucleolus is usually very small, if visible. The nucleoli become more prominent in cancer cells, and sometimes there are multiple.

-   **mitoses**: (1-10). Cancer is essentially a disease of uncontrolled mitosis.

-   **benormal**: (2 or 4). Benign (non-cancerous) or malignant (cancerous) lump in a breast.

#### **II. Materials and Methods**

The University of Wisconsin breast cancer data from William Wolberg has 699 observations and 10 variables, the first variable represents the ID of the sample and the last column “benormal” represents the classification/response variable (for benign, 4 for malignant).

To make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed the response variable values to malignant (4) = 1 and benign (2) = 0, then looked at summary statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).

To identify if there were any patterns among the predictor variables in the dataset, first we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.

![](images/clipboard-1001792189.png){fig-align="center" width="720"}

Some of the variables showed correlations to each other, but none were deemed significant by the corr.test() function. We noticed many of the variables exhibited a right skew with their means larger than their medians. We could potentially log() the variables to make them more uniform.

#### Logistic Regression Model

We developed a logisitic regression model to predict if a cell was cancerous or not. As our outcome can only be one of two things (cell is malignant or benign) we should be using a classification model and logistic regression is a simple model which is much easier to set up and train initially than other machine learning models.

For the first model we used the cleaned dataset and all of the variables.

**Logisitic Regression Model 1-**

```         
Coefficients:
                  Estimate Std. Error z value Pr(>|z|)    
(Intercept)      -10.10394    1.17488  -8.600  < 2e-16 ***
clumpthickness     0.53501    0.14202   3.767 0.000165 ***
uniformcellsize   -0.00628    0.20908  -0.030 0.976039    
uniformcellshape   0.32271    0.23060   1.399 0.161688    
margadhesion       0.33064    0.12345   2.678 0.007400 ** 
epithelial         0.09663    0.15659   0.617 0.537159    
barenuclei         0.38303    0.09384   4.082 4.47e-05 ***
blandchromatin     0.44719    0.17138   2.609 0.009073 ** 
normalnucleoli     0.21303    0.11287   1.887 0.059115 .  
mitoses            0.53484    0.32877   1.627 0.103788    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 102.89  on 673  degrees of freedom
AIC: 122.89
```

![](Rplot.png){fig-align="center" width="530"}

In this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than 0.5. The overall model had an AIC of 122.89.

Further, we created a new model after logging our variables. For this model the variables clumpthickness and barenuclei were considered the only variables that had a significant impact on the response variables with p-values less than 0.5. The overall model had an AIC of 127.02.

**Logisitic Regression Model 2-**

```         
Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)      -9.31304    1.25953  -7.394 1.42e-13 ***
clumpthickness    1.80864    0.59805   3.024  0.00249 ** 
uniformcellsize   0.50056    0.66380   0.754  0.45080    
uniformcellshape  1.26038    0.74379   1.695  0.09016 .  
margadhesion      0.71750    0.39504   1.816  0.06933 .  
epithelial       -0.04541    0.63630  -0.071  0.94310    
barenuclei        0.36117    0.09149   3.948 7.89e-05 ***
blandchromatin    1.49565    0.61321   2.439  0.01473 *  
normalnucleoli    0.48867    0.35560   1.374  0.16938    
mitoses           1.34541    0.74040   1.817  0.06920 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 107.02  on 673  degrees of freedom
AIC: 127.02

Number of Fisher Scoring iterations: 8
```

![](Rplot01.png){fig-align="center" width="530"}

Based on the AIC of these two models, the non-logged model performed better. Then we simplified the model. 

**Logisitic Regression Model 3-**

```         
Coefficients:
                Estimate Std. Error z value Pr(>|z|)    
(Intercept)    -10.11370    1.03264  -9.794  < 2e-16 ***
clumpthickness   0.81166    0.12585   6.450 1.12e-10 ***
margadhesion     0.43412    0.11403   3.807 0.000141 ***
barenuclei       0.48136    0.08816   5.460 4.76e-08 ***
blandchromatin   0.70154    0.15196   4.616 3.90e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 125.77  on 678  degrees of freedom
AIC: 135.77

Number of Fisher Scoring iterations: 8
```

![](Rplot02.png){fig-align="center" width="530"}

As could be predicted, the model that included only the significant variables was worse at explaining the dataset. The complex model is definitely better than the simpler model, as demonstrated by the results of the Chisq test.

```         
Coefficients:
   (Intercept)  clumpthickness    margadhesion      barenuclei  blandchromatin  
      -10.1137          0.8117          0.4341          0.4814          0.7015  

Degrees of Freedom: 682 Total (i.e. Null);  678 Residual
Null Deviance:	    884.4 
Residual Deviance: 125.8 	AIC: 135.8
Analysis of Deviance Table

Model 1: benormal ~ clumpthickness + uniformcellsize + uniformcellshape + 
    margadhesion + epithelial + barenuclei + blandchromatin + 
    normalnucleoli + mitoses
Model 2: benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin
  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
1       673     102.89                          
2       678     125.78 -5  -22.886 0.0003549 ***
```

Finally, evaluating how accurate model_1 is at prediction by using confusion matrix-

```         
glm.pred   0   1
       0 434  11
       1  10 228
```

Looking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).

#### Decision Tree and Random Forest Modeling

Another modeling technique we used for predicting whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it's also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won't happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won't overfit the model.First, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.

**Decision Tree Model 1-**

![](images/clipboard-432770371.png){fig-align="center" width="579"}

Based on the results, it is best to include 6 variables in each split. We created a random forest model at the desired variable split.

![![](images/clipboard-1885854307.png){width="579"}](images/clipboard-1805456869.png){fig-align="center" width="579"}

The variable that is important to reduce the mean standard error (MSE) is barenuceli and the variables that were deemed the most important to include in the node splitting were uniformcellsize and uniformcellshape. Overall, the model is very good with 88.7% of the variables explained. Looking at the confusion matrix:

```         
glm.pred   0   1
       0 438   9
       1   6 230
```

The percentage, the model falsely predicted cancer when not was 6/444 (1.4%) and the percentage, the model falsely predicted not having cancer when there was 9/239 (3.7%).

#### Principal Component Analysis

As previously indicated, while there was some correlation between our variables, none of them was statistically significant. Additionally, some of the factors did not significantly connect with response variable prediction in some of our other models. In order to explore this further, we performed a principal component analysis on the predictor variables to determine whether accuracy naturally decreases when a dataset's variable count is decreased, but not to an excessive degree.

![](images/clipboard-172522986.png){fig-align="center" width="530"}

![](images/clipboard-911580378.png){fig-align="center" width="579"}
