---
title: "FINAL PROJECT CODE"
format:
  html: 
    theme: cosmo
editor: visual
---

```{r}

packages <- c( "randomForest","tree","dplyr", "tidyverse", "plotly", "psych", "ISLR", "leaps", "reshape2","readr")

for (package in packages) {
  if (!(package %in% installed.packages())) {
    install.packages(package, dependencies = TRUE)
  }
}
# Load libraries
suppressMessages({library(dplyr)
library(tidyverse)
library(plotly)
library(psych)
library(ISLR)
library(leaps)
library(reshape2)
library(readr)
library(tree)
library(randomForest)
library(caret)})
```

```{r}
hw = source('hw.R')
url <- "https://raw.githubusercontent.com/KyleWandel/STAT-515-Final-Project/main/breast-cancer-wisconsin.csv"
df <- read.table(url, header = TRUE, sep = ",")
str(df)
```

To make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed he response variable values to malignant (4) = 1 and benign (2) = 0.

After cleaning the dataset, we looked at a summary statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).

```{r}
# remove ID column 
df <- subset(df, select = -id) 
# Change column type
df$barenuclei <- as.integer(df$barenuclei) 
# Look for missing values
colSums(is.na(df)) 
# omit missing values
df_clean <- na.omit(df) 
str(df_clean) 
# Change response variable to 1 and 0
df_clean$benormal <- ifelse(df_clean$benormal == 4, 1, ifelse(df_clean$benormal == 2, 0, df_clean$benormal)) 
summary(df_clean) 
table(df_clean$benormal)
```

We next wanted to identify if there were any patterns amongst the predictor variables in the dataset. First we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.

```{r}
pairs.panels(df_clean)
```

For the first model we used the cleaned dataset and all of the variables.

```{r}
model_1 <- glm(benormal ~ ., data = df_clean, family = binomial) 
summary(model_1) 
par(mfrow = c(2, 2))
plot(model_1)
```

In this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 122.89.

Next, we wanted to create a new model after logging our variables.

```{r}
variables_to_log <- c("mitoses", "normalnucleoli", "blandchromatin", "epithelial", "margadhesion", "uniformcellshape", "uniformcellsize","clumpthickness") 
df_log <- df_clean 
df_log[variables_to_log] <- lapply(df_log[variables_to_log], log) 
model_2 <- glm(benormal ~ ., data = df_log, family = binomial) 
summary(model_2) 
par(mfrow = c(2, 2))
plot(model_2)
```

For this model the variables clumpthickness and barenuclei were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 127.02.

Based on the AIC of these two models, the non-logged model performed better. Now lets try and simplify the model.

```{r}
model_3 <- glm(benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin, data = df_clean, family = binomial) 
summary(model_3) 
model_3 
anova(model_1, model_3, test = "Chisq") 
```

As should be expected, creating a model using only the significant variables was worse at explaining the dataset. Using the Chisq test, we can also conclude that the more complex model is significantly better than the simpler model.

Using model_1 as the final model, lets see how accurate it is for prediction.

```{r}
glm.probs=predict(model_1,type="response")
glm.probs[1:10]

glm.pred=rep(0,nrow(df_clean))
glm.pred[glm.probs>.5]=1

glm.pred=as.factor(glm.pred)
df_clean$benormal=as.factor(df_clean$benormal)
cm = table(glm.pred,df_clean$benormal)
print(cm)
```

Looking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).

### Decision Tree and Random Forest Modeling

Another modeling type we used for trying to predict whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it's also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won't happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won't overfit the model.

First, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.

```{r}
df_clean$benormal=as.integer(df_clean$benormal)
set.seed(2) 
train = sample(1:nrow(df_clean), nrow(df_clean)/2) 
tree.df_clean=tree(benormal~.,df_clean,subset=train) 
cv.df_clean=cv.tree(tree.df_clean) 
plot(cv.df_clean$size,cv.df_clean$dev,type='b')
```

Based on these results, it is best to include 6 variables in each split. Knowing this, we created a random forest model at the desired variable split.

```{r}
df_clean <- na.omit(df) 
df_clean$benormal <- ifelse(df_clean$benormal == 4, 1, ifelse(df_clean$benormal == 2, 0, df_clean$benormal))
cancer_rf=randomForest(benormal~.,data=df_clean,subset=train,mtry=6,importance=TRUE) 
yhat.rf = predict(cancer_rf,newdata=df_clean[-train,]) 
df_clean.test=df_clean[-train,"benormal"] 
mean((yhat.rf-df_clean.test)^2) #test set MSE 
cancer_rf 
plot(cancer_rf) 
varImpPlot(cancer_rf) 
```

The variable that is most important to reduce the mean standard error (MSE) is barenuceli and the variables that were deemed the most important to include in the node splitting were uniformcellsize and uniformcellshape. Overall, the model is very good with 88.7% of the variables explained. Looking at the confusion matrix:

```{r}
glm.probs=predict(cancer_rf,df_clean,type="response")
glm.probs[1:10]

glm.pred=rep(0,nrow(df_clean))
glm.pred[glm.probs>.5]=1

glm.pred=as.factor(glm.pred)
df_clean$benormal=as.factor(df_clean$benormal)
cm1 = table(glm.pred,df_clean$benormal)
print(cm1)
```

The percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%).

### Principal Component Analysis

We mentioned earlier that some of our variables were correlated but none were significantly correlated to each other. Also, in some of our other models, some of the variables were not significantly correlated to predicting the response variable. To examine this further, we did a principal component analysis on the predictor variables too see if reducing the number of variables of a data set naturally comes at the expense of accuracy while not losing too much accuracy.

```{r}
df_clean$benormal <- as.integer(df_clean$benormal)
states=row.names(df_clean)
apply(df_clean, 2, mean)
apply(df_clean, 2, var)
# There isn't much difference in the mean and variance, so we do not need to scale/standardize the variables.
# Calculate the principal components
pr.out=prcomp(df_clean, scale=TRUE)
names(pr.out)
pr.out$center
head(pr.out$x,n = 10)

pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)

(pr.var=pr.out$sdev^2) #variance of each PC
pve=pr.var/sum(pr.var)
pve

# Scree plot
par(mfrow =c(1,2))
plot(pve, xlab="Principal Component", 
     ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')

plot(cumsum(pve), xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1),type='b') + hw

```
